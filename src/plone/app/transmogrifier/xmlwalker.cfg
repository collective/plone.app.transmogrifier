[transmogrifier]
pipeline =
# Initialize the pipeline
    source
    root

# Beginning of the crawling/spidering recursion
    breaker
    deferred
    crawled

# Extract content data from parents
    parent-path
    delete-parent
    remoteUrl

# Retrieve responses from a local cache first, then by requesting a URL
    cache
    log-request
    response
    headers
    cache-headers
    write-cache
    close-response

# Extract content information from response and parsed XML
    parse
    title
    id
    description
    subject
    modificationDate
    content

# Assemble the path and type
    type
    path
    drop-seen
    seen
    unique
    folders
    excludeFromNav

# Parse XML trees for heirarchy/structure and for links to crawl/spider
    walk-content

    relatedItems-tree
    walk-relatedItems
    relatedItems

    left-nav
    walk-left-nav

    nav
    walk-nav

    delete-trees
    
# Send newly crawled/spidered links back to the top, recurse
    log-dropped
    drop
    log-crawled
    crawl

# Add the content
#    blob
    debug
    drop-construct
    log-content
    disable_versioning
    construct
    enable_versioning

# Defer some keys that need redirector entries from other content
    log-deferred
    defer-links
    redirect
    text
    delete-content
    file
    resolve

# Update the content
    update
    close

    transitions
    publish

    defaultpage
    browserdefault

    reindexobject
    savepoint


## Initialize the pipeline with a URL to crawl

#    Start the pipeline with a single CSV row
[source]
blueprint = collective.transmogrifier.sections.csvsource
filename = plone.app.transmogrifier:xmlwalker.csv

#    Set the site root URL to start crawling from
[root]
blueprint = collective.transmogrifier.sections.inserter
key = string:_url
condition = python:item.get('_path') == '/'
url = ${root:scheme}://${root:netloc}${root:path}
value = python:modules['urlparse'].urlsplit(options['url'])


## Central crawling loop
#    This is the source for items deferred until redirections are processed
[deferred]
blueprint = collective.transmogrifier.sections.listsource
#    This is the source for items from links that are crawled
[crawled]
# Emit crawled items before deferred ones
blueprint = collective.transmogrifier.sections.listsource


## Extract content data from parents
# Any parent information that depends on the parent crawling response
# and is needed by it's children, such as paths, must be extracted
# after the walked items to be crawled get sent back to the list
# source because the crawled items must be exhausted before the list
# source will emit the parents

[parent-path]
blueprint = collective.transmogrifier.sections.inserter
key = string:_parent_path
condition = python:'_parent_path' not in item and (\
    '_path' in item.get('_parent', {})\
    or '_parent_path' in item.get('_parent', {}))
value = python:(item['_parent'].get('_type') == 'Folder'\
    and item.get('_parent')['_path'])\
    or item.get('_parent').get(\
        '_parent_path', item.get('_parent').get('_path'))
        
# Free reference to parent
[delete-parent]
blueprint = collective.transmogrifier.sections.manipulator
condition = exists:item/_parent
delete = _parent

[remoteUrl]
blueprint = collective.transmogrifier.sections.inserter
key = string:remoteUrl
condition = python:'remoteUrl' not in item\
    and '_content_element' in item and item.get('_url')\
    and (not item.get('_is_defaultpage')\
         and item['_url'].geturl() in\
         transmogrifier.__annotations__.get('xmlwalker.paths', {}))
# Use Links for nav elements to external URLs or already crawled internal URLs
value = python:(item['_url'].geturl().startswith(transmogrifier['root']['url'])\
    and item['_url'].path) or item['_url'].geturl()


## Get the response for the URL

[cache]
blueprint = collective.transmogrifier.sections.inserter
key = string:_cache
condition = item/_url|nothing
value = python:not modules['os.path'].exists(modules['os.path'].dirname(\
    item.setdefault('_cache', modules['os.path'].join(\
        'var', 'xmlwalker', item['_url'].scheme, item['_url'].netloc,\
        modules['os.path'].dirname(item['_url'].path.lstrip('/')),\
        modules['os.path'].basename(item['_url'].path.lstrip('/'))\
        or 'index.html',)))) and modules['os'].makedirs(\
            modules['os.path'].dirname(item['_cache'])) or item['_cache']

[log-request]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = ${response:condition}
key = _url

[response]
blueprint = collective.transmogrifier.sections.inserter
key = string:_response
condition = python:item.get('_url')\
    and not modules['os.path'].exists(item.get('_cache', ''))
value = python:modules['urllib2'].urlopen(\
    item['_url'].geturl())

# Get response headers from cache if available or populate the cache
[headers]
blueprint = collective.transmogrifier.sections.inserter
key = string:_headers
condition = python:'_cache' in item and '_response' in item
value = python:modules['ConfigParser'].SafeConfigParser(dict(\
    item['_response'].info(), url=item['_response'].geturl(), **dict(
    (name, item['_response'].headers.getparam(name)) for name in\
     item['_response'].headers.getparamnames()))).write(\
        open(item['_cache'] + '.metadata', 'w'))\
    or dict(item['_response'].info())

[cache-headers]
blueprint = collective.transmogrifier.sections.inserter
key = string:_headers
condition = python:'_headers' not in item and '_cache' in item
value = python:item.update(_headers=modules['ConfigParser'].SafeConfigParser())\
    or item['_headers'].read(item['_cache'] + '.metadata')\
    and dict(item['_headers'].defaults())

[write-cache]
blueprint = collective.transmogrifier.sections.inserter
key = string:_write_cache
# writes the cache file
condition = python:('_response' in item and\
        open(item['_cache'], 'w').writelines(item['_response']))
value = nothing

[close-response]
blueprint = collective.transmogrifier.sections.manipulator
condition = python:'_response' in item and item['_response'].close()
delete = _response

[parse]
blueprint = collective.transmogrifier.sections.inserter
key = string:_tree
condition = python:item.get('_headers') and item.get('_cache')\
    and item['_headers'].get('content-type', '').startswith('text/html')
value = python:modules['lxml.etree'].parse(\
    item['_cache'], modules['lxml.html'].HTMLParser(\
        encoding=item.get('_headers', {}).get('charset', 'utf-8')))


## Extract content data from XML trees

[title]
blueprint = collective.transmogrifier.sections.inserter
# Customise xpath to change title extraction
xpath = /html/head/title
# Keep nav title for folders
#   assume items without types will be folders with default pages when crawled
key = python:item.get('_type') == 'Folder' and '_defaultpage' or 'title'
# Keep nav title for links, use /head/title for everything else
condition = ${description:condition} and not item.get('remoteUrl')
value-python = u' '.join(element.text_content().strip() for element in\
                         item['_tree'].xpath(options['xpath']))
value = python:${title:value-python}

[id]
blueprint = plone.app.transmogrifier.urlnormalizer
locale = string:en

[description]
blueprint = collective.transmogrifier.sections.inserter
key = string:description
xpath = /html/head/meta[@name='description']/@content
condition = python:'_tree' in item and item['_tree'].xpath(options['xpath'])
value = python:u' '.join(element.strip() for element in\
    item['_tree'].xpath(options['xpath']))

[subject]
blueprint = collective.transmogrifier.sections.inserter
key = string:subject
xpath = /html/head/meta[@name='keywords']/@content
condition = ${description:condition}
value = python:u','.join(element.strip() for element in \
    item['_tree'].xpath(options['xpath'])).split(',')

[modificationDate]
blueprint = collective.transmogrifier.sections.inserter
key = string:modificationDate
condition = python:'_headers' in item and item['_headers'].get('last-modified')
value = python:item['_headers']['last-modified']

[content]
blueprint = collective.transmogrifier.sections.inserter
key = string:_content
# Don't crawl Folders
condition = python:'_tree' in item and item.get('_type') != 'Folder'\
# Don't crawl Links, also prevents loops on nav Link items
    and 'remoteUrl' not in item
# Customize this xpath expression to isolate the content body elements
xpath = //*[@id='content' or contains(@class, 'content')]/*
value = python:item['_tree'].xpath(options['xpath'])


## Assemble the path and type

[type]
blueprint = collective.transmogrifier.sections.inserter
key = string:_type
# How much of the body to classify,
#   default to the OFS.Image.File linked Pdata chunk size
size = 65536
findTypeName = transmogrifier.context.content_type_registry.findTypeName(\
# Use the extension of the original URL
    getattr(item.get('_url'), 'path', '')\
# Remote URLs should be Links, use a *.url extension
    + (('remoteUrl' in item and not item.get('_is_defaultpage')\
        and item.get('_type') != 'Folder' and '.url') or ''),\
# Use MIME type from respone
    item.get('_headers', {}).get('content-type', ''),\
# Read only some of the file for classification
    item.get('_content',\
        '_cache' in item and open(item['_cache']).read(${type:size})\
        or ''))
condition = python:not item.get('_type') and ${type:findTypeName}
value = python:${type:findTypeName}

[path]
blueprint = collective.transmogrifier.sections.inserter
key = string:_path
condition = python:('_path' not in item\
# Overwrite path for front page
        or item.get('_parent_path') == '/')\
    and item.get('_url')
dirname = item.get('_parent_path', modules['posixpath'].dirname(\
    item.get('_url') and modules['urllib'].unquote(item['_url'].path)\
     or item.get('remoteUrl',\
         item.get('_path', transmogrifier['root']['path']))))
basename = (item.get('_type') not in ('File', 'Image') and item.get('_id'))\
     or modules['posixpath'].basename(\
        item.get('_url') and modules['urllib'].unquote(item['_url'].path)\
        or item.get('remoteUrl', item.get('_path')))
value = python:modules['posixpath'].join(\
# Content links have no structure
    '_content_element' in item and modules['posixpath'].join(\
        ${path:dirname}, modules['posixpath'].dirname(\
            modules['urllib'].unquote(item['_url'].path)))\ 
    or ${path:dirname}, ${path:basename})

[drop-seen]
blueprint = collective.transmogrifier.sections.condition
condition = python:'_url' not in item or (item.get('_path') not in \
    transmogrifier.__annotations__.get('xmlwalker.paths', {}).get(\
        item['_url'].geturl(), set()))

[seen]
blueprint = collective.transmogrifier.sections.inserter
key = string:_seen
condition = python:item.get('_url') and '_path' in item \
    and item.get('_type') != 'Folder'\
# Just add the seen path, don't insert a key
    and transmogrifier.__annotations__.setdefault(\
       'xmlwalker.paths', {}).setdefault(\
           item['_url'].geturl(), set()).add(item['_path'])
value = nothing

[unique]
blueprint = collective.transmogrifier.sections.inserter
key = string:_path
condition = python:item.get('_path', '/') != '/'\
    and transmogrifier.context.restrictedTraverse(\
        item['_path'].lstrip('/'), None) is not None\
# Shoule be a new object if the previous item's path has a different URL
    and (item.get('_url') \
         and item['_url'].geturl() not in\
            transmogrifier.__annotations__.get('xmlwalker.paths', {}))
value = python:modules['posixpath'].join(
    modules['posixpath'].dirname(item['_path']),\
    modules['zope.container.interfaces'].INameChooser(\
        transmogrifier.context.restrictedTraverse(\
            modules['posixpath'].dirname(item['_path']).lstrip('/'))\
        ).chooseName(modules['posixpath'].basename(item['_path']), 
            transmogrifier.context.restrictedTraverse(\
                modules['posixpath'].dirname(item['_path']).lstrip('/'))))

[folders]
blueprint = collective.transmogrifier.sections.folders

[excludeFromNav]
blueprint = collective.transmogrifier.sections.inserter
key = string:excludeFromNav
value = python:True
# Exclude all items from nav if only linked from content
condition = python:'excludeFromNav' not in item and (\
    '_content_element' in item or '_has_element' not in item)


## Get links to crawl from content body next

[walk-content]
blueprint = collective.transmogrifier.sections.xmlwalker
trees = python:'_tree' in item and item.get('_content')
type-key = nothing
is-default-page-key = nothing
element-keys =
    _content_element
    _has_element
    _old_paths
    _parent
    _url
    title

# Content links are always relative to crawled page
element-_parent = tree_item
element-_content_element = item/_has_element|nothing

# Extract information from walked element
old-paths-xpath = (@href | @src)[.!='' and not(starts-with(., '#'))]
element-_old_paths = python:[modules['posixpath'].abspath(\
    modules['posixpath'].join(\
        modules['posixpath'].dirname(tree_item['_url'].path),\
        modules['urlparse'].urlsplit(href.strip()).path))\
    for href in element.xpath(\
        transmogrifier['walk-content']['old-paths-xpath'])\
    if modules['urlparse'].urljoin(\
        tree_item['_url'].geturl(), href.strip()).startswith(\
            transmogrifier['root']['url'])]
element-_url = python:item['_old_paths'] and (item.update(\
    _url=modules['urlparse'].urlsplit(item['_old_paths'][0]))\
    or modules['urlparse'].SplitResult(\
    item['_url'].scheme or transmogrifier['root']['scheme'],\
    item['_url'].netloc or transmogrifier['root']['netloc'],\
    modules['urllib'].quote(item['_old_paths'][0]), '', ''))
element-title = python:unicode(element.text_content().strip()\
                               or element.attrib.get('alt', '').strip())
element-_has_element = python:len(element)


## Get related items from first left nav

[relatedItems-tree]
blueprint = collective.transmogrifier.sections.inserter
key = string:_relatedItems
condition = python:False
xpath = //*[contains(@class, 'nav-list')][2]
value = python:item['_tree'].xpath(options['xpath'])

[walk-relatedItems]
blueprint = collective.transmogrifier.sections.xmlwalker
trees = item/_relatedItems|nothing
type-key = ${walk-content:type-key}
is-default-page-key = ${walk-content:is-default-page-key}
element-keys = ${walk-content:element-keys}

# related items links are always relative to crawled page
element-_parent = ${walk-content:element-_parent}
element-_content_element = ${walk-content:element-_content_element}

element-_old_paths = ${walk-content:element-_old_paths}
element-_url = ${walk-content:element-_url}
element-title = ${walk-content:element-title}
element-_has_element = ${walk-content:element-_has_element}

[relatedItems]
blueprint = collective.transmogrifier.sections.inserter
key = string:relatedItems
condition = python:'relatedItems' not in item and '_relatedItems' in item\
    and item['_tree'].xpath("${relatedItems:xpath}")
xpath = (${relatedItems-tree:xpath}//*/@href\
    | ${relatedItems-tree:xpath}//*/@src)[.!='' and not(starts-with(., '#'))]
value = python:[modules['posixpath'].abspath(\
    modules['posixpath'].join(\
        item['_parent_path'],\
        modules['urlparse'].urlsplit(href.strip()).path))\
    for href in item['_tree'].xpath("${relatedItems:xpath}")]


## Get links to crawl from left navigation list

[left-nav]
blueprint = collective.transmogrifier.sections.inserter
key = string:_left_nav
condition = ${content:condition}
# Customize this xpath expression to isolate the left navigation elements
xpath = //*[contains(@class, 'nav-list')][1]
value = python:item['_tree'].xpath(options['xpath'])

[walk-left-nav]
blueprint = collective.transmogrifier.sections.xmlwalker
trees = item/_left_nav|nothing
cache = true
element-keys =
    _has_element
    _old_paths
    _url
    title

element-_old_paths = ${walk-content:element-_old_paths}
element-_url = ${walk-content:element-_url}
element-title = ${walk-content:element-title}
element-_has_element = ${walk-content:element-_has_element}


## Get links to crawl from site nav first

[nav]
blueprint = collective.transmogrifier.sections.inserter
key = string:_nav
condition = ${content:condition}
# Customize this xpath expression to isolate the navigation elements
xpath = //*[contains(@class, 'navbar')]//ul[contains(@class, 'nav')]
value = python:item['_tree'].xpath(options['xpath'])

[walk-nav]
blueprint = collective.transmogrifier.sections.xmlwalker
trees = item/_nav|nothing
cache = true
element-keys = ${walk-left-nav:element-keys}

element-_old_paths = ${walk-content:element-_old_paths}
element-_url = ${walk-content:element-_url}
element-title = ${walk-content:element-title}
element-_has_element = ${walk-content:element-_has_element}


## Free/remove memory intensive XML tree references
[delete-trees]
blueprint = collective.transmogrifier.sections.manipulator
delete =
    _tree
    _nav
    _left_nav
    _relatedItems


## Send child items back to the top of the crawl loop

[log-dropped]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = not:${drop:condition}
key = _url

[drop]
blueprint = collective.transmogrifier.sections.condition
ignored-extensions =
condition = python:'_url' not in item or '_element' not in item or (\
# Drop external links in content, let links in nav become Link content
    (item['_url'].geturl().startswith(transmogrifier['root']['url'])\
    or not '_content_element' in item)\
# skip certain file types
    and modules['posixpath'].splitext(\
        item['_url'].path or '_.html')[1] not in [\
        ext.strip() for ext in options['ignored-extensions'].split()\
        if ext.strip()]\
# Don't re-process previously processed links in content
    and (not '_content_element' in item\
         or item['_url'].geturl()\
         not in transmogrifier.__annotations__.get('xmlwalker.paths', {})))

[log-crawled]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = ${crawl:condition}
key = _url

[crawl]
blueprint = collective.transmogrifier.sections.listappender
condition = python:'_cache' not in item and item.get('_url')
section = crawled


## Add the content

[drop-construct]
blueprint = collective.transmogrifier.sections.manipulator
# Do not add content for the portal itself
condition = python:item.get('_path') == '/'
delete = _type

[log-content]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = python:item.get('_path') and item.get('_type')
key = _path

[disable_versioning]
blueprint = plone.app.transmogrifier.versioning.disable

[construct]
blueprint = collective.transmogrifier.sections.constructor

[enable_versioning]
blueprint = plone.app.transmogrifier.versioning.enable


## Defer some keys that need redirector entries from other content

[log-deferred]
blueprint = collective.transmogrifier.sections.logger
level = INFO
condition = ${defer-links:condition}
key = _path

[defer-links]
blueprint = collective.transmogrifier.sections.listappender
condition = python:item.get('_url') and (\
    item.get('relatedItems') or '_content' in item)
section = deferred
keys = python:['title', 'description', 'subject', 'modificationDate',\
    'excludeFromNav', '_content', 'relatedItems', 'remoteUrl']
copy-keys = python:['_path', '_content_element', '_has_element']

[redirect]
blueprint = plone.app.transmogrifier.redirector


[text]
blueprint = collective.transmogrifier.sections.inserter
key = string:text
condition = python:item.get('_type') != 'Folder' and '_content' in item\
    and 'remoteUrl' not in item
value = python:u'\n'.join([modules['lxml.etree'].tostring(\
        element, method='html', encoding=unicode, pretty_print=True)\
    for element in item['_content']])

[delete-content]
blueprint = collective.transmogrifier.sections.manipulator
condition = exists:item/_content
delete = _content

# Open the cache as a real file for the File and Image types fields
[file]
blueprint = collective.transmogrifier.sections.inserter
key = python:(item.get('_type') == 'Image' and 'image')\
    or (item.get('_type') == 'File' and 'file')
condition = python:key and 'text' not in item and '_cache' in item
value = python:open(item['_cache'])

[resolve]
blueprint = collective.transmogrifier.sections.pathresolver
keys = relatedItems

[update]
blueprint = plone.app.transmogrifier.atschemaupdater

# Close and delete file references to avoid too many open files
[close]
blueprint = collective.transmogrifier.sections.manipulator
condition = python:('file' in item and item['file'].close())\
    or ('image' in item and item['image'].close()) or True
delete =
    file
    image


[transitions]
blueprint = collective.transmogrifier.sections.inserter
key = string:_transitions
# Assume all crawled links should be published
value = python:["publish"]
condition = python:'_url' in item\
    and item.get('_type') not in ('File', 'Image')

[publish]
blueprint = plone.app.transmogrifier.workflowupdater
transitions = submit publish


[defaultpage]
blueprint = plone.app.transmogrifier.urlnormalizer
source-key = _defaultpage
destination-key = string:_defaultpage
locale = string:en

[browserdefault]
blueprint = plone.app.transmogrifier.browserdefault


[reindexobject]
blueprint = plone.app.transmogrifier.reindexobject

[savepoint]
blueprint = collective.transmogrifier.sections.savepoint


## Debugging tools

[debug]
blueprint = collective.transmogrifier.sections.logger
# Change to True to log full items for debugging
condition = python:False
level = INFO
delete = text

[breaker]
blueprint = collective.transmogrifier.sections.breakpoint
# Change to True to log full items for debugging
condition = python:False
